{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATE TWITTER TROLL DETECTION USING TRANSFORMERS\n",
    "\n",
    "# INTRO + BACKGROUND\n",
    "\n",
    "With the 2020 US election around the corner, the issue of [electoral interference](https://www.nytimes.com/2020/09/10/us/politics/russian-hacking-microsoft-biden-trump.html) by state actors via social media and other online means is back in the spotlight. Twitter was a major platform by which Russia sought to interfere with the 2016 US election, and few have doubts that Moscow, Beijing and others will return to the platform with new disinformation campaigns.\n",
    "\n",
    "This series of notebooks will show an end-to-end walkthrough of how you can build a state Twitter troll detector by fine tuning a transformer model with a custom dataset. This builds on my earlier project using \"classic\" machine learning models and a simple bag-of-words approach to detect [state troll] tweets(https://github.com/chuachinhon/twitter_state_trolls_cch).\n",
    "\n",
    "That earlier version worked well under certain conditions. If the model was trained on Russian troll tweets, for instance, it would be pretty good at picking out new, unseen Russian troll tweets which the model had not been trained on. But if you used it to try to pick out state troll tweets of Iranian or Venezuelian origin, the accuracy dips very significantly. Fuller discussion in my Medium post [here](https://towardsdatascience.com/using-data-science-to-uncover-state-backed-trolls-on-twitter-dc04dc749d69). \n",
    "\n",
    "The BOW approach is clearly outdated. I would expect the transformer models to perform much better. But can it generalise well? Meaning, in this particular context, whether a model fine tuned on Russian and Chinese state troll tweets can also effectively detect troll tweets by Iranian or Saudi operators? This is what I hope to find out at the end of the project.\n",
    "\n",
    "## MODEL\n",
    "\n",
    "For this project, I'll be fine tuning the [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased). Feel free to switch out to another model of your choice.\n",
    "\n",
    "\n",
    "## CUSTOM DATASET\n",
    "\n",
    "If you want to skip straight to the fine tuning part, go to notebook2.0 in this repo. I've provided the custom training(file name: [train_raw.csv](https://github.com/chuachinhon/transformers_state_trolls_cch/blob/master/data/train_raw.csv)) and validation datasets(file name: [validate.csv](https://github.com/chuachinhon/transformers_state_trolls_cch/blob/master/data/validate.csv)), and you can start from there. \n",
    "\n",
    "Notebooks 1.0-1.2 outline how I collected, assembled and re-sampled the custom dataset. I reckon not everyone will be interested or have the patience for this.\n",
    "\n",
    "\n",
    "## RAW DATASETS/GROUND TRUTH\n",
    "\n",
    "* Troll tweets: The state troll tweets used in this project are those flagged by Twitter, so that establishes our \"ground truth\" in terms of what constitutes a state troll tweet. The original CSV files are too big to be uploaded to Github, so you'll have to download them directly from [Twitter](https://transparency.twitter.com/en/reports/information-operations.html) if you wish to run this notebook or build a bigger sample. I've used six raw files from Twitter to build a sample set of 50K troll tweets. The six sets of state troll tweets comprise those by:\n",
    "\n",
    "- Russia, 3 sets released in May 2020, Jan 2019, and Oct 2018.\n",
    "- China, 3 sets released in May 2020, and Aug 2019.\n",
    "\n",
    "* Real tweets: I scraped about 317K tweets from 175 verified users and accounts that I personally checked for authenticity. I've provided a sample of the Tweepy notebook, so feel free to scrape your own tweets if you prefer (you need your own auth keys). I won't provide the raw scraped files, but the concated CSV file with all 317k + files is here.\n",
    "\n",
    "\n",
    "## REPO STRUCTURE\n",
    "\n",
    "### 1. DATA FOLDER\n",
    "\n",
    "* 5 CSV files for notebooks in this series. Note that raw troll tweet files from Twitter are not included here.\n",
    "\n",
    "### 2. NOTEBOOKS FOLDER\n",
    "\n",
    "* Notebooks 1.0 - 1.2: Data collection, cleaning and preparation. Optional if you just want to experiment with the final dataset.\n",
    "\n",
    "* Notebooks 2.0 - 2.1: Fine tuning distilbert with custom dataset and detailed testing with unseen validation dataset, as well as a fresh dataset with state troll tweets from Iran.\n",
    "\n",
    "* Notebook 3.0 - 3.1: Create and test optimised logistic regression and XGB models against datasets used to assess fine tuned Distilbert model.\n",
    "\n",
    "\n",
    "### 3. APP FOLDER\n",
    "\n",
    "* app.py + folders for \"static\" and \"template: simple app for use on a local machine to demonstrate how a state troll tweet detector can be used in deployment. Unfortunately free hosting accounts can't accomodate the disk size required for pytorch and the fine tuned model, so I've not deployed this online. \n",
    "\n",
    "\n",
    "### 4. TROLL_DETECT FOLDER\n",
    "\n",
    "* Fine tuned Distilbert model from Colab notebook2.0. Too big for Github, download [here](https://www.dropbox.com/sh/90h7ymog2oi5yn7/AACTuxmMTcso6aMxSmSiD8AVa) from Dropbox instead.\n",
    "\n",
    "### 5. PKL FOLDER\n",
    "\n",
    "* Pickled logistic regression model from notebook3.0 \n",
    "\n",
    "* Pickled XGB model from notebook3.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1A: TROLL TWEETS COLLECTION, CLEANING AND PREPARATION\n",
    "\n",
    "In this notebook, we'll deal with the state troll tweets as identified by Twitter. I'm only using those from China and Russia, as they are the two biggest state actors in this area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these datasets are huge and NOT in the repo. \n",
    "# Download them here: https://transparency.twitter.com/en/reports/information-operations.html\n",
    "\n",
    "# Mainland Chinese state troll tweets\n",
    "raw1 = pd.read_csv(\"../data/china_052020_tweets.csv\").dropna(subset=[\"tweet_text\"])\n",
    "raw2 = pd.read_csv(\"../data/china_082019_1_tweets_csv_hashed.csv\").dropna(\n",
    "    subset=[\"tweet_text\"]\n",
    ")\n",
    "raw3 = pd.read_csv(\"../data/china_082019_2_tweets_csv_hashed.csv\").dropna(\n",
    "    subset=[\"tweet_text\"]\n",
    ")\n",
    "\n",
    "# Russian state troll tweets\n",
    "raw4 = pd.read_csv(\"../data/russia_052020_tweets.csv\").dropna(subset=[\"tweet_text\"])\n",
    "raw5 = pd.read_csv(\"../data/russian_linked_tweets_csv_hashed.csv\").dropna(\n",
    "    subset=[\"tweet_text\"]\n",
    ")\n",
    "raw6 = pd.read_csv(\"../data/ira_tweets_csv_hashed.csv\").dropna(subset=[\"tweet_text\"])\n",
    "\n",
    "china_raw = pd.concat([raw1, raw2, raw3])\n",
    "\n",
    "russia_raw = pd.concat([raw4, raw5, raw6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: FILTER OUT NON-ENGLISH TWEETS AND RETWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm focusing only on English tweets. Retweets also filtered out\n",
    "\n",
    "crit1 = china_raw[\"tweet_language\"] == \"en\"\n",
    "crit2 = china_raw[\"is_retweet\"] == False\n",
    "crit3 = ~china_raw[\"tweet_text\"].str.startswith(\"RT@\")\n",
    "\n",
    "china_raw = china_raw[crit1 & crit2 & crit3].copy()\n",
    "\n",
    "crit4 = russia_raw[\"tweet_language\"] == \"en\"\n",
    "crit5 = russia_raw[\"is_retweet\"] == False\n",
    "crit6 = ~russia_raw[\"tweet_text\"].str.startswith(\"RT@\")\n",
    "\n",
    "russia_raw = russia_raw[crit4 & crit5 & crit6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: CLEAN TWEET TEXT\n",
    "\n",
    "I'm removing links, mentions, digits, but keeping hashtags. Depending on your use case, you might want to revise the cleaning regex rules below.\n",
    "\n",
    "I'm also dropping tweets which have fewer than 3 words after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text) # Change 't to 'not'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text) # Remove @name\n",
    "    text = re.sub(r\"$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$\", \" \", text) # remove digits\n",
    "    text = re.sub(r\"[^\\w\\s\\#]\", \"\", text) #remove special characters except hashtags\n",
    "    text = text.strip(\" \")\n",
    "    text = re.sub(' +',' ', text).strip() # get rid of multiple spaces and replace with a single\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_raw[\"clean_text\"] = (\n",
    "    china_raw[\"tweet_text\"]\n",
    "    .map(lambda text: clean_text(text))\n",
    ")\n",
    "\n",
    "russia_raw[\"clean_text\"] = (\n",
    "    russia_raw[\"tweet_text\"]\n",
    "    .map(lambda text: clean_text(text))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_raw['word_count'] = china_raw['clean_text'].str.count(' ') + 1\n",
    "\n",
    "russia_raw['word_count'] = russia_raw['clean_text'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit7 = ~china_raw[\"clean_text\"].isnull()\n",
    "crit8 = china_raw[\"clean_text\"] != \"\"\n",
    "crit9 = china_raw[\"word_count\"] > 3\n",
    "\n",
    "china_raw = china_raw[crit7 & crit8 & crit9].copy()\n",
    "\n",
    "crit10 = ~russia_raw[\"clean_text\"].isnull()\n",
    "crit11 = russia_raw[\"clean_text\"] != \"\"\n",
    "crit12 = russia_raw[\"word_count\"] > 3\n",
    "\n",
    "russia_raw = russia_raw[crit10 & crit11 & crit12].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['tweetid', 'user_display_name', 'tweet_text','clean_text']\n",
    "\n",
    "china = china_raw[cols].copy()\n",
    "\n",
    "russia = russia_raw[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: if you want to save the full set for future samples\n",
    "#china.to_csv(\"../data/china_trolls_full.csv\", index=False)\n",
    "#russia.to_csv(\"../data/russia_trolls_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: CREATE 50K TROLL TWEETS SAMPLE\n",
    "\n",
    "To make the fine tuning process more manageable (time and resource-wise), I decided to sample a smaller slice of the fuller troll tweets data instead of using it in full. If you have access to a tonne of GPUs, feel free to use a bigger slice of the troll tweets data.\n",
    "\n",
    "### For this project, troll tweets will be labelled 1, while real tweets will be labelled 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_sample = china.sample(n=25000, random_state=42, replace=False)\n",
    "\n",
    "russia_sample = russia.sample(n=25000, random_state=42, replace=False)\n",
    "\n",
    "troll_sample = pd.concat([china_sample, russia_sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "troll_sample[\"troll_or_not\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "troll_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>user_display_name</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>troll_or_not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>332962</th>\n",
       "      <td>1245883557362282497</td>\n",
       "      <td>85c9M6CDZxgBwoEye0rF12ZBgGl3xvz6Bnbvhp7MUKI=</td>\n",
       "      <td>having each tiny wish come true, or having som...</td>\n",
       "      <td>having each tiny wish come true or having some...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417285</th>\n",
       "      <td>961577921461866496</td>\n",
       "      <td>曲剑明</td>\n",
       "      <td>＠null It is 12:25 UTC now</td>\n",
       "      <td>null It is UTC now</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992674</th>\n",
       "      <td>941616158075211776</td>\n",
       "      <td>IFL1E0m0SRX2cdOtuLFV7xKtnBgxagKzNgkuGFvNtvs=</td>\n",
       "      <td>British number two Bedene to switch back to Sl...</td>\n",
       "      <td>British number two Bedene to switch back to Sl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135543</th>\n",
       "      <td>850414479976345600</td>\n",
       "      <td>Klausv</td>\n",
       "      <td>kalamitykait Thanks for bearing with us - you ...</td>\n",
       "      <td>kalamitykait Thanks for bearing with us you sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463169</th>\n",
       "      <td>960784360071925760</td>\n",
       "      <td>曲剑明</td>\n",
       "      <td>＠null It is 08:56 CET now</td>\n",
       "      <td>null It is CET now</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    tweetid                             user_display_name  \\\n",
       "332962  1245883557362282497  85c9M6CDZxgBwoEye0rF12ZBgGl3xvz6Bnbvhp7MUKI=   \n",
       "417285   961577921461866496                                           曲剑明   \n",
       "992674   941616158075211776  IFL1E0m0SRX2cdOtuLFV7xKtnBgxagKzNgkuGFvNtvs=   \n",
       "135543   850414479976345600                                        Klausv   \n",
       "463169   960784360071925760                                           曲剑明   \n",
       "\n",
       "                                               tweet_text  \\\n",
       "332962  having each tiny wish come true, or having som...   \n",
       "417285                          ＠null It is 12:25 UTC now   \n",
       "992674  British number two Bedene to switch back to Sl...   \n",
       "135543  kalamitykait Thanks for bearing with us - you ...   \n",
       "463169                          ＠null It is 08:56 CET now   \n",
       "\n",
       "                                               clean_text  troll_or_not  \n",
       "332962  having each tiny wish come true or having some...             1  \n",
       "417285                                 null It is UTC now             1  \n",
       "992674  British number two Bedene to switch back to Sl...             1  \n",
       "135543  kalamitykait Thanks for bearing with us you sh...             1  \n",
       "463169                                 null It is CET now             1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "troll_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've included this dataset in the repo in case you want an even smaller slice of the dataset\n",
    "\n",
    "\"\"\"\n",
    "troll_sample.to_csv(\n",
    "    \"../data/troll_50k.csv\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8\",\n",
    "    quoting=csv.QUOTE_NONNUMERIC,\n",
    ")\n",
    "\"\"\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
